{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCFRWf_cqb0Y"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2023 William Locke\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqIqipcjq-Ao"
      },
      "outputs": [],
      "source": [
        "# NEON Tree Crowns / Evaluation Dataset copyright (c) 2023 weecology\n",
        "\n",
        "# This work is licensed under CC BY 4.0\n",
        "\n",
        "# NEON Tree Crowns website: https://www.weecology.org/data-projects/neon-crowns/\n",
        "# NEON Tree Evaluation Github: https://github.com/weecology/NeonTreeEvaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NsqvmTpkamN"
      },
      "source": [
        "This notebook is intended to be run in Google Colab with access to corresponding Google Drive files. If running locally or on another service, change import and install code accordingly.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lu-liang-geo/UAV_Tree_Detection/blob/main/notebooks/NEON_Tree_Evaluation_EDA.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjeieLzO-wmt"
      },
      "source": [
        "**NEON Tree Training Dataset**\n",
        "\n",
        "There are only 16 images in the NEON training set, but they have significant variation in size and number of trees captured. The smallest image is 888 x 1153 pixels while the largest (five) images are 10,000 x 10,000 pixels. Most are in the low thousands of pixels per side. (These sizes are for the RGB images; the Hyperspectral and CHM images are all roughly 10x smaller than their corresponding RGB images, but cover the same area just with a lower resolution.)\n",
        "\n",
        "The differing sizes and different tree densities also results in different numbers of trees being captured per image. Most images are in the hundreds to low thousands (less than 2000), but the two lowest-count images have 1 and 40 trees respectively, and the two highest have 3670 and 9730 respectively. Bizarrely, the low-tree outliers are two of the largest images (10,000 x 10,000), and the high-tree outliers are smaller images.\n",
        "\n",
        "All NEON images seem to have roughly the same spatial resolution as each other, and this resolution is slightly less than our images (i.e. a single pixel covers more area in the NEON dataset, resulting in less details captured). I'm not sure the actual range of spatial resolutions in the dataset.\n",
        "\n",
        "All of this makes me wonder if we should reconsider cropping our training data to fixed or at least smaller size before passing it to the model, and if so what size that should be with what overlap. It would require additional work to make sure the proper bounding boxes get assigned to the proper image crops."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLcHI9rnYEPs"
      },
      "source": [
        "**NEON Tree Evaluation Dataset**\n",
        "\n",
        "There are roughly 200 images in the NEON evaluation dataset, and they all have a uniform size of 400 x 400 for the RGB channels and 40 x 40 for the Hyperspectral and CHM channels (so the same 10x size difference between RGB and other channels as in the training set). The evaluation images are drawn from a larger set of forests than the training images, though there is some overlap between them (in terms of forests, presumably not individual images). I'm not sure if some of the evaluation images are cropped from larger, contiguous images, and if so whether they could be recombined back into larger images (or if there would be any advantage to doing so)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4QJQVU7rEm4"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Euro8eAlbBF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ2XepXcxQQi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/training.zip' -d \"/content/training\"\n",
        "!unzip '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/evaluation.zip' -d \"/content\"\n",
        "!unzip '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/annotations.zip' -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFKlatpdylco"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio\n",
        "!pip install supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RluDvyvRyq1n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80up64IX-s0p"
      },
      "source": [
        "## NEONTreeDataset class definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbolaem08rVU"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This class definition is similar to the one found in Github, but it also gives access to the raw values of\n",
        "several image channels for analysis purposes.\n",
        "'''\n",
        "\n",
        "class NEONTreeDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, image_path, ann_path, prompt_path=None, check_values=False):\n",
        "    '''\n",
        "    params\n",
        "      image_path (str): Path to top-level training image directory (contains RGB, LiDAR, Hyperspectral, CHM)\n",
        "      ann_path (str):  Path to annotations\n",
        "\n",
        "    __init__ stores filenames from RGB folder, which will be used to retrieve relevant files from other\n",
        "    folders. Because RGB folder has four files not found in Hyperspectral or CHM, we remove any entries\n",
        "    not found in these other folders. We also manually remove four images we consider unsuitable for\n",
        "    training / evaluation, either because they have a large number of invalid pixels or they don't have\n",
        "    good annotations.\n",
        "    '''\n",
        "    self.check_values = check_values\n",
        "    self.image_path = image_path\n",
        "    self.ann_path = ann_path\n",
        "    self.prompt_path = prompt_path\n",
        "    file_names = os.listdir(os.path.join(image_path, 'RGB'))\n",
        "    problem_files = set(['2019_SJER_4_251000_4103000_image', '2019_TOOL_3_403000_7617000_image', 'TALL_043_2019', 'SJER_062_2018'])\n",
        "    basenames = [name.split('.')[0] for name in file_names]\n",
        "    basenames = [name for name in basenames if name not in problem_files]\n",
        "    basenames = [name for name in basenames if os.path.exists(os.path.join(image_path, 'Hyperspectral', f'{name}_hyperspectral.tif'))]\n",
        "    basenames = [name for name in basenames if os.path.exists(os.path.join(ann_path,f'{name}.xml'))]\n",
        "    basenames = [name for name in basenames if os.path.exists(os.path.join(image_path, 'CHM', f'{name}_CHM.tif'))]\n",
        "    self.basenames = list(set(basenames))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.basenames)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    returns\n",
        "      annotated_image (dict) with keys:\n",
        "        rgb_img: HxWxC ndarray of RGB channels\n",
        "        multi_img: HxWxC ndarray of multi channels\n",
        "        prompt (if prompt_path specified): Nx4 ndarray of prompt bounding boxes in XYXY format\n",
        "        annotation: Nx4 ndarray of ground truth bounding boxes in XYXY format\n",
        "\n",
        "        ndvi_raw: HxWxC ndarray of NDVI values, with NaN values replaced by 4 to aid in visualization\n",
        "\n",
        "\n",
        "    Currently I hardcode the multi_img channels, but later I may allow the user to specify them in __init__\n",
        "    '''\n",
        "    basename = self.basenames[idx]\n",
        "    rgb_path = os.path.join(self.image_path, 'RGB', f'{basename}.tif')\n",
        "    chm_path = os.path.join(self.image_path, 'CHM', f'{basename}_CHM.tif')\n",
        "    hs_path = os.path.join(self.image_path, 'Hyperspectral', f'{basename}_hyperspectral.tif')\n",
        "    ann_path = os.path.join(self.ann_path, f\"{basename}.xml\")\n",
        "    if self.prompt_path:\n",
        "      prompt_path = os.path.join(self.prompt_path, 'Boxes', f\"{basename}.npy\")\n",
        "    annotated_image = {'basename': basename}\n",
        "\n",
        "    # Open RGB path and two paths used to construct multi image. Save rgb_img in annotated_image.\n",
        "    with rasterio.open(rgb_path) as img:\n",
        "      rgb_img = img.read().transpose(1,2,0)\n",
        "    with rasterio.open(hs_path) as img:\n",
        "      hs_img = img.read()\n",
        "    with rasterio.open(chm_path) as img:\n",
        "      chm_img = img.read()\n",
        "    annotated_image['rgb'] = rgb_img\n",
        "\n",
        "    # Remove blank rows or columns from edges of CHM and Hyperspectral images, based on null value of -9999.0 in CHM.\n",
        "    if chm_img[0,0,1]==-9999.0:\n",
        "      chm_img = chm_img[:,1:,:]\n",
        "      hs_img = hs_img[:,1:,:]\n",
        "    if chm_img[0,1,0]==-9999.0:\n",
        "      chm_img = chm_img[:,:,1:]\n",
        "      hs_img = hs_img[:,:,1:]\n",
        "    if chm_img[0,-1,-2]==-9999.0:\n",
        "      chm_img = chm_img[:,:-1,:]\n",
        "      hs_img = hs_img[:,:-1,:]\n",
        "    if chm_img[0,-2,-1]==-9999.0:\n",
        "      chm_img = chm_img[:,:,:-1]\n",
        "      hs_img = hs_img[:,:,:-1]\n",
        "\n",
        "    assert not (chm_img==-9999.0).any()\n",
        "\n",
        "    # Select NIR, Red, and Red-Edge channels based on frequency reference chart\n",
        "    # https://github.com/weecology/NeonTreeEvaluation/blob/master/neon_aop_bands.csv\n",
        "    # Note that reference chart starts at 1 while we start at 0, so add 1 to the numbers below to get corresponding chart row.\n",
        "    nir = 95\n",
        "    red = 53\n",
        "    edge = 69\n",
        "\n",
        "    # Extract NIR, Red, and Red-Edge channels from Hyperspectral Image. Make CHM single channel.\n",
        "    nir_img = hs_img[nir]\n",
        "    red_img = hs_img[red]\n",
        "    edge_img = hs_img[edge]\n",
        "    chm_img = chm_img[0]\n",
        "\n",
        "    # Check for any negative values in NIR, Red, Red-Edge, and CHM channels, which are likely invalid pixels.\n",
        "    if self.check_values:\n",
        "      assert rgb_img.min() >= 0, f'{basename} RGB values below 0 (min value: {rgb_img.min()}), check source image'\n",
        "      assert nir_img.min() >= 0, f'{basename} NIR values below 0 (min value: {nir_img.min()}), check source image.'\n",
        "      assert red_img.min() >= 0, f'{basename} Red values below 0 (min value: {red_img.min()}), check source image.'\n",
        "      assert edge_img.min() >= 0, f'{basename} Red Edge values below 0 (min value: {edge_img.min()}), check source image.'\n",
        "      assert chm_img.min() >= 0, f'{basename} Canopy Height values below 0 (min value: {chm_img.min()}), check source image.'\n",
        "\n",
        "    # Set NaN and values less than 0 equal to 0. This allows for processing images with invalid pixel values\n",
        "    # in some channels (we specifically allow this in 2019_OSBS_5 in the training set).\n",
        "    rgb_img[rgb_img<0] = 0\n",
        "    nir_img[nir_img<0] = 0\n",
        "    red_img[red_img<0] = 0\n",
        "    edge_img[edge_img<0] = 0\n",
        "    chm_img = np.nan_to_num(chm_img, nan=0.0)\n",
        "\n",
        "    # Save non-standardized channel values for analysis\n",
        "    annotated_image['nir_raw'] = nir_img\n",
        "    annotated_image['red_raw'] = red_img\n",
        "    annotated_image['edge_raw'] = edge_img\n",
        "    annotated_image['chm_raw'] = chm_img\n",
        "\n",
        "    # Create NDVI from NIR and Red channels. NaN values (where NIR and Red are both 0) converted to 0 for multi_img,\n",
        "    # but set to 4 for ndvi_raw to highlight locations where NDVI can't be calculated normally.\n",
        "    _ndvi_img = (nir_img - red_img) / (nir_img + red_img)\n",
        "    ndvi_img = np.nan_to_num(_ndvi_img, nan=0)\n",
        "    if not np.isfinite(_ndvi_img).all():\n",
        "      ndvi_raw = np.nan_to_num(_ndvi_img, nan=4)\n",
        "      annotated_image['ndvi_raw'] = ndvi_raw\n",
        "\n",
        "    # Standardize Red-Edge channel\n",
        "    edge_img = (edge_img - edge_img.mean()) / edge_img.std()\n",
        "\n",
        "    # Standardize CHM channel\n",
        "    chm_img = (chm_img - chm_img.mean()) / chm_img.std()\n",
        "\n",
        "    # Create multi-channel image of chm, NDVI, and Red-Edge, save in annotated_image.\n",
        "    multi_img = np.stack([chm_img, ndvi_img, edge_img], axis=-1).astype('float32')\n",
        "    annotated_image['multi'] = multi_img\n",
        "\n",
        "    # If Prompt Boxes have already been generated (and self.prompt_path is not None), load prompt boxes\n",
        "    # and save in annotated_image.\n",
        "    if self.prompt_path:\n",
        "      prompt = np.load(prompt_path)\n",
        "      annotated_image['prompt'] = prompt\n",
        "\n",
        "    # Extract bounding boxes from annotations, save in annotated_image.\n",
        "    xyxy = []\n",
        "    tree = ET.parse(ann_path)\n",
        "    root = tree.getroot()\n",
        "    for obj in root.findall('object'):\n",
        "      name = obj.find('name').text\n",
        "      if name == 'Tree':\n",
        "        bbox = obj.find('bndbox')\n",
        "        xyxy.append([int(bbox[i].text) for i in range(4)])\n",
        "    annotation = np.array(xyxy)\n",
        "    annotated_image['annotation'] = annotation\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "  def get_image(self, basename, return_index=False):\n",
        "    index = self.basenames.index(basename)\n",
        "    if return_index:\n",
        "      return index\n",
        "    else:\n",
        "      return self.__getitem__(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGr2knU8Ng3c"
      },
      "outputs": [],
      "source": [
        "train_ds = NEONTreeDataset('/content/training', '/content/annotations', check_values=False)\n",
        "val_ds = NEONTreeDataset('/content/evaluation', '/content/annotations', check_values=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0HB2fVn97i7"
      },
      "source": [
        "## Image Stats\n",
        "\n",
        "As mentioned above, both the training and validation datasets have different sized images (RGB and Multi) and different numbers of trees. This next code block saves text summaries of these statistics, both for individual images and for forest groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCwBkKDH3U0F"
      },
      "outputs": [],
      "source": [
        "#@title Save Image Names, Sizes, Num Trees, and Forest Groups\n",
        "\n",
        "train_maps = defaultdict(int)\n",
        "train_trees = defaultdict(int)\n",
        "\n",
        "# Save text description of dataset RGB image sizes and tree counts\n",
        "f = open('NEON_train.txt', 'w')\n",
        "f.write(f'{\"Filename\":<45} {\"RGB Image Size\":<20} {\"Multi Image Size\":<20} {\"Num Trees\"}\\n')\n",
        "for img in train_ds:\n",
        "  # Write individual image stats\n",
        "  basename = img['basename']\n",
        "  rgb_img = img['rgb']\n",
        "  multi_img = img['multi']\n",
        "  boxes = img['annotation']\n",
        "  f.write(f'{basename:<45} {str(rgb_img.shape[:-1]):<20} {str(multi_img.shape[:-1]):<20} {boxes.shape[0]}\\n')\n",
        "\n",
        "  # Record group stats\n",
        "  group_name = [name for name in basename.split('_') if name.isupper()][0]\n",
        "  train_maps[group_name] += 1\n",
        "  train_trees[group_name] += boxes.shape[0]\n",
        "f.close()\n",
        "\n",
        "# Write group stats\n",
        "f = open('NEON_train_groups.txt', 'w')\n",
        "f.write(f'{\"Group Name\":<15} {\"Num Maps\":<10} {\"Num Trees\"}\\n')\n",
        "for key in train_maps.keys():\n",
        "  f.write(f'{key:<15} {train_maps[key]:<10} {train_trees[key]}\\n')\n",
        "f.close()\n",
        "\n",
        "\n",
        "val_maps = defaultdict(int)\n",
        "val_trees = defaultdict(int)\n",
        "\n",
        "# Save text description of dataset RGB image sizes and tree counts\n",
        "f = open('NEON_eval.txt', 'w')\n",
        "f.write(f'{\"Filename\":<45} {\"RGB Image Size\":<20} {\"Multi Image Size\":<20} {\"Num Trees\"}\\n')\n",
        "for img in val_ds:\n",
        "  # Write individual image stats\n",
        "  basename = img['basename']\n",
        "  rgb_img = img['rgb']\n",
        "  multi_img = img['multi']\n",
        "  boxes = img['annotation']\n",
        "  f.write(f'{basename:<45} {str(rgb_img.shape[:-1]):<20} {str(multi_img.shape[:-1]):<20} {boxes.shape[0]}\\n')\n",
        "\n",
        "  # Record group stats\n",
        "  group_name = [name for name in basename.split('_') if name.isupper()][0]\n",
        "  val_maps[group_name] += 1\n",
        "  val_trees[group_name] += boxes.shape[0]\n",
        "f.close()\n",
        "\n",
        "# Write group stats\n",
        "f = open('NEON_eval_groups.txt', 'w')\n",
        "f.write(f'{\"Group Name\":<15} {\"Num Maps\":<10} {\"Num Trees\"}\\n')\n",
        "for key in val_maps.keys():\n",
        "  f.write(f'{key:<15} {val_maps[key]:<10} {val_trees[key]}\\n')\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj7Y71P9-gCb"
      },
      "source": [
        "## Show Images\n",
        "\n",
        "The following code blocks allow a user to view any RGB image in the chosen dataset, as well as any channel in the Multi image.\n",
        "\n",
        "The user can specify the image by index in the dataset, using `img = ds[index]` (note that this index is subject to change each time the dataset is initialized), or by name using the method `img = ds.get_image(name)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqdDFcNzB8MP"
      },
      "outputs": [],
      "source": [
        "#@title Choose Image\n",
        "\n",
        "# Choose dataset, train_ds or val_ds\n",
        "ds = val_ds\n",
        "\n",
        "# Choose image, x = an integer for choosing by index or x = a string for choosing by image name\n",
        "x = 'DELA_047_2019'\n",
        "\n",
        "if isinstance(x, int):\n",
        "  img = ds[x]\n",
        "else:\n",
        "  img = ds.get_image(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV5YwmzABZA8"
      },
      "outputs": [],
      "source": [
        "#@title Show RGB Image\n",
        "\n",
        "# Choose whether to include ground truth annotations with image\n",
        "annotations = True\n",
        "box_annotator = sv.BoxAnnotator(thickness=2, color=sv.Color.red())\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "rgb_img = img['rgb']\n",
        "boxes = img['annotation']\n",
        "if annotations:\n",
        "  boxes = sv.Detections(xyxy=boxes, confidence=np.ones(len(boxes)))\n",
        "  bgr_img = box_annotator.annotate(scene=rgb_img[:,:,::-1].copy(), detections=boxes, skip_label=True)\n",
        "  rgb_img = bgr_img[:,:,::-1]\n",
        "plt.imshow(rgb_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkfPcijzSFdl"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Canopy Height Model\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "chm_img = img['multi'][:,:,0]\n",
        "plt.imshow(chm_img, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvBi5VgmS1eZ"
      },
      "outputs": [],
      "source": [
        "#@title Visualize Red-Edge Channel\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "edge_img = img['multi'][:,:,2]\n",
        "plt.imshow(edge_img, cmap='viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuQ--0LEU19h"
      },
      "source": [
        "For some images, there are pixels where both the Red channel and the NIR channel have a value of zero. Because of this, attempting to calculate `NDVI = (NIR - Red) / (NIR + Red)` returns a division-by-zero error, and these pixels are assigned a NaN value. For processing the Multi image, I simply replace these values with 0, which is the midpoint of possible NDVI values (-1 to 1). However, I'm still not sure this is the most appropriate solution. In the code block below, if there are pixels where NDVI cannot be calculated, there will be two visualizations -- one with those pixels filled in with a value of 0, as they are in the Multi image, and one with those pixels filled in with a value of 4, which causes them to clearly stand out against a much darker background. The two visualizations can then be compared to see what pixels have been filled in with 0 in the Multi image and whether this seems an appropriate choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RM_AG6TTM7v"
      },
      "outputs": [],
      "source": [
        "#@title Visualize NDVI Channel\n",
        "if 'ndvi_raw' in img.keys():\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(8,16))\n",
        "  for ax in axs:\n",
        "    ax.axis('off')\n",
        "  ndvi = img['multi'][:,:,1]\n",
        "  ndvi_raw = img['ndvi_raw']\n",
        "  axs[0].imshow(ndvi, cmap='viridis')\n",
        "  axs[1].imshow(ndvi_raw, cmap='viridis')\n",
        "  fig.tight_layout()\n",
        "  fig.show()\n",
        "else:\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.axis('off')\n",
        "  ndvi = img['multi'][:,:,1]\n",
        "  plt.imshow(ndvi)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPUOmW5r7Qzr"
      },
      "source": [
        "## Multi Channel Statistics\n",
        "\n",
        "The Multi image is composed of three channels: Canopy Height Model (CHM), Normalized Difference Vegetation Index (NDVI), and Red-edge. We selected these based on their availability in the data and what we thought might be most helpful for distinguishing individual trees from each other and from other objects.\n",
        "\n",
        "The Multi image has three channels because it is being passed through the SAM encoder, which only accepts images with three channels (usually RGB). In the future, it might be worth training a new encoder to handle Multi images separately, but for now we are passing both RGB and Multi images through the same encoder, and so they must have the same number of channels (more on this below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0Wmck14bzrr"
      },
      "outputs": [],
      "source": [
        "# Collect all NIR, Red, CHM, and Red-edge raw image values\n",
        "nir_raw = np.stack([img['nir_raw'] for img in ds])\n",
        "red_raw = np.stack([img['red_raw'] for img in ds])\n",
        "chm_raw = np.stack([img['chm_raw'] for img in ds])\n",
        "edge_raw = np.stack([img['edge_raw'] for img in ds])\n",
        "\n",
        "# Collect all NDVI image values after NaN replacement and\n",
        "# all CHM and Red-edge image values after standardization\n",
        "chm_all = np.stack([img['multi'][:,:,0] for img in ds])\n",
        "ndvi_all = np.stack([img['multi'][:,:,1] for img in ds])\n",
        "edge_all = np.stack([img['multi'][:,:,2] for img in ds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8K_CuCIvcth"
      },
      "source": [
        "The SAM encoder has its own saved values used to standardize RGB values, but these will not work for the Multi images. As a result, we must standardize them ourselves. The code block below shows the values of these Multi channels before and after standardization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gp5-FNFcPbt"
      },
      "outputs": [],
      "source": [
        "#@title Multi Channel Values\n",
        "print('BEFORE Standardization')\n",
        "print(f'{\"CHM\":<10} Max Value: {chm_raw.max():>7.2f}    Min Value: {chm_raw.min():>5.2f}    Mean Value: {chm_raw.mean():>7.2f}    STD: {chm_raw.std():>7.2f} \\n')\n",
        "print(f'{\"NIR\":<10} Max Value: {nir_raw.max():>7.2f}    Min Value: {nir_raw.min():>5.2f}    Mean Value: {nir_raw.mean():>7.2f}    STD: {nir_raw.std():>7.2f} \\n')\n",
        "print(f'{\"Red\":<10} Max Value: {red_raw.max():>7.2f}    Min Value: {red_raw.min():>5.2f}    Mean Value: {red_raw.mean():>7.2f}    STD: {red_raw.std():>7.2f} \\n')\n",
        "print(f'{\"Red Edge\":<10} Max Value: {edge_raw.max():>7.2f}    Min Value: {edge_raw.min():>5.2f}    Mean Value: {edge_raw.mean():>7.2f}    STD: {edge_raw.std():>7.2f} \\n')\n",
        "print()\n",
        "print('After Standardization')\n",
        "print(f'{\"CHM\":<10} Max Value: {chm_all.max():>7.2f}    Min Value: {chm_all.min():>5.2f}    Mean Value: {chm_all.mean():>7.2f}    STD: {chm_all.std():>7.2f} \\n')\n",
        "print(f'{\"NDVI\":<10} Max Value: {ndvi_all.max():>7.2f}    Min Value: {ndvi_all.min():>5.2f}    Mean Value: {ndvi_all.mean():>7.2f}    STD: {ndvi_all.std():>7.2f} \\n')\n",
        "print(f'{\"Red Edge\":<10} Max Value: {edge_all.max():>7.2f}    Min Value: {edge_all.min():>5.2f}    Mean Value: {edge_all.mean():>7.2f}    STD: {edge_all.std():>7.2f} \\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSvb9Xbkvuwy"
      },
      "source": [
        "We want to know if the channels in the Multi image carry unique information, so we aren't simply repeating the same information across three channels. Calculating the Correlation Matrix between CHM, NDVI, and Red-edge channels shows that they are not strongly correlated, meaning each channel should be carrying unique information from the others. In fact, if we compare this to the RGB channels, they are much more strongly correlated than the Multi channels. (However, because we are passing both images through the same encoder, this could also mean that the encoder would have a harder time interpreting the Multi image; more on this below.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB8YgItBkaXI"
      },
      "outputs": [],
      "source": [
        "#@title Multi Channel Correlation Matrix\n",
        "\n",
        "chm_flat = chm_all.flatten()\n",
        "ndvi_flat = ndvi_all.flatten()\n",
        "edge_flat = edge_all.flatten()\n",
        "\n",
        "multi_channels = np.stack([chm_flat, ndvi_flat, edge_flat])\n",
        "r_multi = np.corrcoef(multi_channels)\n",
        "print('Multi Channel Correlation Matrix \\n')\n",
        "print(r_multi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaXSZImSmVPj"
      },
      "outputs": [],
      "source": [
        "rgb_all = np.stack([img['rgb'] for img in ds])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDTKrWotkj28"
      },
      "outputs": [],
      "source": [
        "red_flat = rgb_all[...,0].flatten()\n",
        "green_flat = rgb_all[...,1].flatten()\n",
        "blue_flat = rgb_all[...,2].flatten()\n",
        "\n",
        "rgb_channels = np.stack([red_flat, green_flat, blue_flat])\n",
        "r_rgb = np.corrcoef(rgb_channels)\n",
        "print('RGB Channel Correlation Matrix \\n')\n",
        "print(r_rgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhU-yd__xc0_"
      },
      "source": [
        "## SAM Encoder on Multi Images\n",
        "\n",
        "As stated above, both the RGB and Multi images are passed through the same SAM encoder, which was trained on roughly 11 million images and so is a very powerful tool for extracting meaningful representations of images. However, it was trained exclusively on RGB images, and so it's questionable whether the representations it creates of non-RGB images would be as useful. Ultimately, this will have to be experimentally tested with our own Box Decoder model, which will learn to interpret the outputs of the SAM encoder both on RGB and Multi images. However, as an early proxy test, we can see how SAM's Mask Decoder (which was also trained exclusively on RGB images) handles the encodings of Multi images vs RGB images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lny8sWuSzj5C"
      },
      "outputs": [],
      "source": [
        "# Copy SAM from personal github repository\n",
        "\n",
        "%%capture\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists('/content/UAV_Tree_Detection'):\n",
        "  !rm -r /content/UAV_Tree_Detection\n",
        "!git clone https://github.com/lu-liang-geo/UAV_Tree_Detection.git\n",
        "%cd /content/UAV_Tree_Detection\n",
        "!pip install -q .\n",
        "!mkdir /content/weights\n",
        "%cd /content/weights\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbV8AmwJ2HvT"
      },
      "outputs": [],
      "source": [
        "from segment_and_detect_anything import sam_model_registry, SamPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SogfoJBC2Exg"
      },
      "outputs": [],
      "source": [
        "# Initialize SAM model and predictor\n",
        "sam_model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/weights/sam_vit_h_4b8939.pth\")\n",
        "sam_predictor = SamPredictor(sam_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FqIpuiP2qaz"
      },
      "outputs": [],
      "source": [
        "def segment(sam_predictor: SamPredictor, boxes: np.ndarray) -> np.ndarray:\n",
        "    result_masks = []\n",
        "    for box in boxes:\n",
        "        masks, scores, logits = sam_predictor.predict(\n",
        "            box=box,\n",
        "            output_type='mask',\n",
        "            multimask_output=False\n",
        "        )\n",
        "        index = np.argmax(scores)\n",
        "        result_masks.append(masks[index])\n",
        "    return np.array(result_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SrdwlES6Mm_"
      },
      "outputs": [],
      "source": [
        "# Embed RGB image and Multi image\n",
        "\n",
        "sam_predictor.set_images(img['rgb'], img['multi'])\n",
        "rgb_features, multi_features = sam_predictor.get_image_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMPmw0mH6rt8"
      },
      "outputs": [],
      "source": [
        "#@title Segmentation with RGB embeddings\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(thickness=2, color=sv.Color.red())\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "rgb_img = img['rgb']\n",
        "boxes = img['annotation']\n",
        "rgb_masks = segment(sam_predictor, boxes)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "detections = sv.Detections(xyxy=boxes, confidence=np.ones(len(boxes)), mask=rgb_masks, class_id=np.zeros(len(boxes), dtype=np.int64))\n",
        "bgr_img = box_annotator.annotate(scene=rgb_img[:,:,::-1].copy(), detections=detections, skip_label=True)\n",
        "bgr_img = mask_annotator.annotate(scene=bgr_img.copy(), detections=detections)\n",
        "rgb_img = bgr_img[:,:,::-1]\n",
        "plt.imshow(rgb_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ctpMqMrY1lk"
      },
      "outputs": [],
      "source": [
        "#@title Segmentation with Multi embeddings\n",
        "\n",
        "sam_predictor.features = multi_features\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(thickness=2, color=sv.Color.red())\n",
        "mask_annotator = sv.MaskAnnotator()\n",
        "\n",
        "rgb_img = img['rgb']\n",
        "boxes = img['annotation']\n",
        "rgb_masks = segment(sam_predictor, boxes)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.axis('off')\n",
        "detections = sv.Detections(xyxy=boxes, confidence=np.ones(len(boxes)), mask=rgb_masks, class_id=np.zeros(len(boxes), dtype=np.int64))\n",
        "bgr_img = box_annotator.annotate(scene=rgb_img[:,:,::-1].copy(), detections=detections, skip_label=True)\n",
        "bgr_img = mask_annotator.annotate(scene=bgr_img.copy(), detections=detections)\n",
        "rgb_img = bgr_img[:,:,::-1]\n",
        "plt.imshow(rgb_img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq1o6mP5cGWY"
      },
      "source": [
        "Somewhat surprisingly, the Mask Decoder is able to interpret the Multi image embedding and outputs similar masks to those it outputs when relying on the RGB image embedding, though with a bit less accuracy. This suggests that the embeddings are similar enough to be useful in making the same sorts of predictions, despite encoding different information. Of course, this is a very small test -- it might be worth experimenting with other images and other combinations of channels to see if this similarity holds."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qPUOmW5r7Qzr"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
