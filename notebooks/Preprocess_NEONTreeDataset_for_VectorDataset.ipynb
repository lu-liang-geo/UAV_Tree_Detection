{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfAuXapIhaXL"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) 2023 William Locke\n",
        "\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PmCI8BAikML"
      },
      "source": [
        "This notebook is intended to be run in Google Colab with access to corresponding Google Drive files. If running locally or on another service, change import and install code accordingly.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WilliamLockeIV/segment-anything/blob/main/notebooks/Preprocess_NEONTreeDataset_for_VectorDataset.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r9s0nDtTf_1"
      },
      "source": [
        "The purpose of this notebook is to take raw image and bounding box information saved in the NEONTreeDataset and encode it into vectors can be saved in a VectorDataset. See the ReadMe for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Euro8eAlbBF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4WKtvnbnI"
      },
      "source": [
        "TODO: Preprocess NEONTreeEvaluation training set.\n",
        "\n",
        "Thus far we have only worked with the evaluation set, in which all images are cropped to a uniform size of 400 x 400 pixels. We still need to decide whether to try training our model on the full-size training images, which can range from 888 x 1153 pixels to 10,000 x 10,000 pixels, or also crop these into smaller images of 400 x 400 pixels. We will eventually need to make the same decisions for our own dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ2XepXcxQQi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/training.zip' -d \"/content/training\"\n",
        "!unzip '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/evaluation.zip' -d \"/content\"\n",
        "!unzip '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/annotations.zip' -d \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFKlatpdylco"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio\n",
        "!pip install supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJqX0mC0qUgX"
      },
      "outputs": [],
      "source": [
        "#@title Copy GroundingDINO from IDEA-Research github repository\n",
        "%%capture\n",
        "\n",
        "%cd /content\n",
        "import os\n",
        "if not os.path.exists('/content/weights'):\n",
        "  !mkdir /content/weights\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd /content/GroundingDINO\n",
        "!pip install -q .\n",
        "%cd /content/weights\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urMJJesIoXAb"
      },
      "outputs": [],
      "source": [
        "#@title Copy SAM from personal github repository\n",
        "%%capture\n",
        "\n",
        "%cd /content\n",
        "import os\n",
        "if os.path.exists('/content/segment-anything'):\n",
        "  !rm -r /content/segment-anything\n",
        "!git clone https://github.com/lu-liang-geo/UAV_Tree_Detection.git\n",
        "%cd /content/segment-anything\n",
        "!pip install -q .\n",
        "%cd /content/weights\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RluDvyvRyq1n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import supervision as sv\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from segment_and_detect_anything.detr import box_ops\n",
        "from GroundingDINO.groundingdino.util.inference import Model\n",
        "from segment_and_detect_anything import NEONTreeDataset, sam_model_registry, SamPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKKACENU5dT5"
      },
      "outputs": [],
      "source": [
        "#@title Load GroundingDINO Model\n",
        "GROUNDING_DINO_CONFIG_PATH = \"/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\"\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = \"/content/weights/groundingdino_swinb_cogcoor.pth\"\n",
        "gd_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5E8c_reu4OS"
      },
      "outputs": [],
      "source": [
        "#@title Load SAM Model\n",
        "sam_model = sam_model_registry[\"vit_h\"](checkpoint=\"/content/weights/sam_vit_h_4b8939.pth\")\n",
        "sam_predictor = SamPredictor(sam_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXHg3CCOplak"
      },
      "source": [
        "NOTE: when creating a dataset for the first time with a new set of images, set `check_values=True` to raise an error if any RGB, NIR, Red Edge, or CHM pixels are less than 0, which likely indicates invalid pixels are included in the image. These images may be further cropped to remove the invalid pixels or eliminated from the dataset altogether by adding them to the set `problem_files` inside the code for NEONTreeDataset.\n",
        "\n",
        "ALSO NOTE: We have already preprocessed the val dataset; I include it here for instructional and archival purposes only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGr2knU8Ng3c"
      },
      "outputs": [],
      "source": [
        "#@title Initialize either Train or Val Dataset\n",
        "\n",
        "mode = # 'train' or 'val'\n",
        "\n",
        "if mode == 'train':\n",
        "  img_path = '/content/training'\n",
        "  prompt_path = None\n",
        "\n",
        "elif mode == 'val':\n",
        "  img_path = '/content/evaluation'\n",
        "  prompt_path = '/content/drive/MyDrive/UAV/Data/NEONTreeEvaluation/Evaluation/Prompts'\n",
        "\n",
        "ann_path = '/content/annotations'\n",
        "\n",
        "ds = NEONTreeDataset(image_path=img_path, ann_path=ann_path, prompt_path=prompt_path, check_values=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OL_pDXs8wL0y"
      },
      "outputs": [],
      "source": [
        "#@title Run Through Train and Val Datasets\n",
        "\n",
        "# The first time running through the val dataset takes about 1 minute; subsequent runthroughs are much faster.\n",
        "# I run through the dataset here so that they won't add to the time when encoding the data below.\n",
        "\n",
        "for tree in ds:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcExkwkdvRJR"
      },
      "source": [
        "## Encode RGB and Multi Images using SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EJ8gFOYA923"
      },
      "source": [
        "CAUTION: After encoding and saving the RGB and Multi images using SAM, you should open a new notebook (or at least disconnect and delete the current runtime and start a new one) and run the SAM encoder on one or two of the same images to compare its outputs with the saved embeddings. For some reason the first time I encoded the RGB and Multi images with SAM, the saved embeddings were different from the ones output by SAM in a later notebook, and I had to rerun all the images through SAM and save the new embeddings. I still haven't figured out why this happened."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZi-fSYDvVrf"
      },
      "outputs": [],
      "source": [
        "#@title Encode Images from Scratch\n",
        "\n",
        "rgb_folder = '/content/path_to_rgb_vector_folder'\n",
        "multi_folder = '/content/path_to_multi_vector_folder'\n",
        "\n",
        "for i, tree in enumerate(ds):\n",
        "  rgb_img = tree['rgb']\n",
        "  multi_img = tree['multi']\n",
        "  name = tree['basename']\n",
        "  sam_predictor.set_images(rgb_img, multi_img)\n",
        "  rgb_embed, multi_embed = sam_predictor.get_image_embedding()\n",
        "  torch.save(rgb_embed, os.path.join(rgb_folder, f'{name}.pt'))\n",
        "  torch.save(multi_embed, os.path.join(multi_folder, f'{name}.pt'))\n",
        "  print(i+1, name)\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF5LlRSlyhTu"
      },
      "outputs": [],
      "source": [
        "#@title If continuing from previous encoding session\n",
        "\n",
        "rgb_folder = '/content/path_to_rgb_vector_folder'\n",
        "multi_folder = '/content/path_to_multi_vector_folder'\n",
        "\n",
        "rgb_encoded = set(os.listdir(rgb_folder))\n",
        "multi_encoded = set(os.listdir(multi_folder))\n",
        "imgs_encoded = rgb_encoded.intersection(multi_encoded)\n",
        "\n",
        "print('Done:', len(imgs_encoded), 'To Do:', len(ds) - len(imgs_encoded))\n",
        "print()\n",
        "\n",
        "i = 0\n",
        "for tree in ds:\n",
        "  name = tree['basename']\n",
        "  if not name in imgs_encoded:\n",
        "    rgb_img = tree['rgb']\n",
        "    multi_img = tree['multi']\n",
        "    sam_predictor.set_images(rgb_img, multi_img)\n",
        "    rgb_embed, multi_embed = sam_predictor.get_image_embedding()\n",
        "    torch.save(rgb_embed, os.path.join(rgb_folder, f'{name}.pt'))\n",
        "    torch.save(multi_embed, os.path.join(multi_folder, f'{name}.pt'))\n",
        "    print(i+1, name)\n",
        "    i += 1\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0py4wDq0tEb"
      },
      "source": [
        "## Encoding Prompt Boxes using GroundingDINO and SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL-TyTwbCzxl"
      },
      "source": [
        "The code block below raises the following warnings, which I have not yet addressed in the code:\n",
        "\n",
        "```\n",
        "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
        "  warnings.warn(\n",
        "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
        "  warnings.warn(\n",
        "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "  warnings.warn(\n",
        "```\n",
        "The final warning is not an issue, but the deprecation of `device` might cause an error in the future, and the changing default value of `use_reentrant` might change some behavior, though I don't really understand this parameter. The relevant documenation is currently available at https://pytorch.org/docs/stable/checkpoint.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkq9SpBD0shL"
      },
      "outputs": [],
      "source": [
        "#@title Generate Bounding Boxes with GroundingDINO (can skip if already done)\n",
        "\n",
        "'''\n",
        "Classes are the categories GroundingDINO will attempt to find in the image. Box threshold is the\n",
        "confidence threshold necessary to draw a box, and text_theshold is the confidence threshold necessary\n",
        "to attach a label to that box. See GroundingDINO documentation for more details.\n",
        "\n",
        "Early tests of GroundingDINO found a simple class of \"tree\" with box_threshold and text_threshold set\n",
        "to 0.2 worked best in conjunction with Custom Non-Max Suppression.\n",
        "'''\n",
        "\n",
        "box_folder = '/content/path_to_box_folder'\n",
        "\n",
        "classes = ['tree']\n",
        "box_threshold = 0.2\n",
        "text_threshold = 0.2\n",
        "\n",
        "for i, tree in enumerate(ds):\n",
        "  tree = ds[i]\n",
        "  name = tree['basename']\n",
        "  bgr_img = tree['rgb'][:,:,::-1].copy()\n",
        "  prompt_boxes = gd_model.predict_with_classes(\n",
        "      image=bgr_img,\n",
        "      classes=classes,\n",
        "      box_threshold=box_threshold,\n",
        "      text_threshold=text_threshold)\n",
        "  prompt_boxes = box_ops.custom_nms(prompt_boxes).xyxy\n",
        "  np.save(os.path.join(box_folder, f'{name}.npy'), prompt_boxes)\n",
        "  print(i, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U1ZWsr4DKfG"
      },
      "outputs": [],
      "source": [
        "#@title Add Bounding Boxes to Dataset (can skip if already done)\n",
        "\n",
        "ds = NEONTreeDataset(image_path=img_path, ann_path=ann_path, prompt_path=box_folder, check_values=False)\n",
        "\n",
        "for tree in ds:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNc4mojQB2LZ"
      },
      "source": [
        "We calculate and save **sparse**, **dense**, and **positional** embeddings using the SAM prompt encoder.\n",
        "\n",
        "**Sparse embeddings** are the embeddings of the bounding boxes provided by GroundingDINO. It would also include any point prompts if provided, but those do not apply to our dataset.\n",
        "\n",
        "**Dense embeddings** encode mask prompts rather than box prompts and aren't used by our Box Decoder (nor can they be due to how the Box Decoder reshapes prompt embeddings). We save them anyway so that we can run the same embeddings through the Mask Decoder to check that it outputs reasonable masks, which is a good indicator that we haven't made any mistakes in the preprocessing and embedding.\n",
        "\n",
        "**Positional embeddings** only depend on the size of the encoded image, so we save a single embedding per image size rather than per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxDVvcHCyXwm"
      },
      "outputs": [],
      "source": [
        "#@title Encode Bounding Box Prompts as Sparse, Dense, and Positional Embeddings\n",
        "\n",
        "sparse_folder = '/content/path_to_sparse_folder'\n",
        "dense_folder = '/content/path_to_dense_folder'\n",
        "positional_folder = '/content/path_to_positional_folder'\n",
        "\n",
        "# Save Sparse and Dense embeddings\n",
        "for i, tree in enumerate(ds):\n",
        "  rgb_img = tree['rgb_img']\n",
        "  boxes = tree['prompt']\n",
        "  name = tree['basename']\n",
        "  boxes_transform = sam_predictor.transform.apply_boxes(boxes, rgb_img.shape[:2])\n",
        "  boxes_torch = torch.as_tensor(boxes_transform, dtype=torch.float)\n",
        "  sparse_embedding, dense_embedding = sam_model.prompt_encoder(\n",
        "      points=None,\n",
        "      boxes=boxes_torch,\n",
        "      masks=None\n",
        "  )\n",
        "  torch.save(sparse_embedding, os.path.join(sparse_folder, f'{name}.pt'))\n",
        "  torch.save(dense_embedding, os.path.join(dense_folder, f'{name}.pt'))\n",
        "  print(i, name)\n",
        "\n",
        "# Save positional embedding for encoded image size\n",
        "# (should be the same for all images so long as they are encoded by the same SAM model).\n",
        "image_embedding_size = sam_model.prompt_encoder.image_embedding_size\n",
        "positional_embedding = sam_model.prompt_encoder.get_dense_pe()\n",
        "torch.save(positional_embedding, os.path.join(positional_folder, f'{image_embedding_size}.pt'))\n",
        "print()\n",
        "print(image_embedding_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Bk8JugKxn2"
      },
      "source": [
        "## Encoding Annotations and Class Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsGDoK1_K680"
      },
      "source": [
        "The NEONTreeDataset annotations are saved in PASCAL VOC format, which are XML documents saving (among other things) the bounding boxes as Xmin, Ymin, Xmax, Ymax coordinates in pixels. To train the Box Decoder, we need to normalize the pixel values by the original image size and convert them to Center_X, Center_Y, Width, Height format. We do this here.\n",
        "\n",
        "We also create a \"Class Label\". For the NEONTreeDataset, there is only a single class (\"Tree\"), so the class label will be 0 for each object (later the model will add a non-object class of 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOue-XQcMwdV"
      },
      "outputs": [],
      "source": [
        "#@title Encode Annotation Bounding Boxes in CxCyWH format\n",
        "\n",
        "annotation_folder = '/content/path_to_annotations'\n",
        "\n",
        "for i, tree in enumerate(tree_ds):\n",
        "  name = tree['basename']\n",
        "  h, w = tree['rgb'].shape[:-1]\n",
        "  orig_box = torch.from_numpy(tree['annotation'])\n",
        "  resize_box = box_ops.box_xyxy_to_cxcywh(orig_box) / torch.Tensor([w,h,w,h])\n",
        "  torch.save(resize_box, os.path.join(annotation_folder, f'{name}.pt'))\n",
        "  print(i, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRY0fqqtS0u0"
      },
      "outputs": [],
      "source": [
        "#@title Encode Class Labels\n",
        "\n",
        "label_folder = '/content/path_to_label_folder'\n",
        "\n",
        "for i, tree in enumerate(tree_ds):\n",
        "  name = tree['basename']\n",
        "  num_boxes = len(tree['annotation'])\n",
        "  labels = torch.zeros(num_boxes, dtype=torch.int64)\n",
        "  torch.save(labels, os.path.join(label_folder, f'{name}.pt'))\n",
        "  print(i, name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
